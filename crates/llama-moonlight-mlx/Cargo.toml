[package]
name = "llama-moonlight-mlx"
version = "0.1.0"
edition = "2021"
authors = ["Your Name <your.email@example.com>"]
description = "MLX integration for AI-powered browser automation in Llama-Moonlight"
repository = "https://github.com/yourusername/llama-ecosystem"
license = "MIT OR Apache-2.0"
keywords = ["browser", "automation", "ai", "mlx", "llm"]
categories = ["web-programming", "computer-vision", "machine-learning"]

[dependencies]
llama-moonlight-core = { path = "../llama-moonlight-core", version = "0.1.0" }
tokio = { version = "1.32", features = ["full"] }
anyhow = "1.0"
thiserror = "1.0"
log = "0.4"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
image = "0.24"
base64 = "0.21"
url = "2.4"
regex = "1.9"
lazy_static = "1.4"
bytes = "1.5"
async-trait = "0.1"
# candle-core is a placeholder for MLX integration
candle-core = { version = "0.3", optional = true }
candle-nn = { version = "0.3", optional = true }
tokenizers = { version = "0.14", optional = true }

[features]
default = []
# The full feature enables all machine learning capabilities
full = ["text", "vision", "audio"]
# Enable text processing features (LLMs)
text = ["dep:candle-core", "dep:candle-nn", "dep:tokenizers"]
# Enable computer vision features
vision = ["dep:candle-core", "dep:candle-nn"]
# Enable audio processing features
audio = ["dep:candle-core", "dep:candle-nn"]

[dev-dependencies]
tempfile = "3.8"
pretty_assertions = "1.3" 